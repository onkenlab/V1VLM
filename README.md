# V1VLM - An vision language model for analyzing neural coding in primary visual cortex

Code for the V1VLM model, a vision language model using
[ViV1T](https://github.com/bryanlimy/ViV1T-closed-loop) as a digital twin of
primary visual cortex, based on Google's Gemma-3 and Black Forest Labs'
FLUX.1.


## Installation

After setting up [ViV1T](https://github.com/bryanlimy/ViV1T-closed-loop)
install the following additional required packages:
```
pip install diffusers sentencepiece transformers accelerate markdown-pdf
```


## Usage

Use a text context file to tell the model about relevant literature.
Then start a study for a given context file:
```
python run.py --context-file "./context.txt"
```
The model will come up with new neural coding hypotheses, generate input
images, run the digital twin to produce neuronal responses,
analyze the findings and conclude the study with a PDF report.


## Example report generated by the model

The following report was generated by the model. As context, we provided the introduction from the paper "[Movie-trained transformer reveals novel response properties to dynamic stimuli in mouse visual cortex](https://www.biorxiv.org/content/10.1101/2025.09.16.676524)".


### Report: Investigating Relational Encoding in Mouse Primary Visual Cortex – A Deep Neural Network Approach

**1. Introduction & Idea Explored:**

This research investigates the hypothesis that mouse primary visual cortex (V1) neurons represent visual information not solely through the encoding of low-level features like orientation and spatial frequency, as previously assumed, but through the integration of *relational* spatial properties – specifically, the geometry of intersecting shapes and the relationships between these elements.  This builds upon recent advancements in computational neuroscience that utilize deep neural networks to predict neuronal responses to natural stimuli, offering a powerful tool to explore complex neural mechanisms [kietzmann et al., 2017; Richards et al., 2019].  Previous work has predominantly focused on generating “most exciting images” (MEIs) to elicit maximal responses, often leading to relatively simplistic representations [bashivan et al., 2019; walker et al., 2019]. This study aims to move beyond MEI generation by directly manipulating spatial relationships within generated images and observing the resulting neuronal responses, seeking evidence of a more sophisticated encoding scheme.  We hypothesize that V1 neurons aren't simply responding to individual features but to the relational properties – like intersecting lines, angles, and distortions – that define complex visual forms.

**2. Experiments:**

We employed a core-readout deep neural network architecture, inspired by recent successes in V1 modelling [klindt et al., 2017; lurz et al., 2021]. The network consists of a convolutional core responsible for extracting latent features from static images and a linear readout layer that maps these features to individual neurons.  The core was trained end-to-end using data from a large population of simulated neurons, mirroring the data-driven approach used in previous studies [antolik et al., 2016; burg et al., 2021].  The following experiment was conducted:

* **Stimulus Generation:**  We generated a series of grayscale images with progressively increasing spatial complexity. Starting with a layered topographical map <img src="figures/input_image_1.png" alt="Generated Input Image 1">, the stimuli were then manipulated to include closely spaced, drifting, and irregularly shaped lines <img src="figures/input_image_2.png" alt="Generated Input Image 2">, followed by a network of intersecting asymmetrical shapes <img src="figures/input_image_3.png" alt="Generated Input Image 3">. These images were designed to systematically vary the density, orientation, and spatial arrangement of elements.
* **Neuronal Response Simulation:** The generated images were fed into the trained core-readout network, which simulated the activity of a large population of V1 neurons.  The resulting neuronal response traces were then recorded for analysis.
* **Analysis:** We analyzed the recorded neuronal responses using a combination of techniques, including:
    * **Time-Locked Averaging:**  Calculating average activity across the entire neuron population for each stimulus.
    * **Waveform Analysis:** Examining the shape and timing of individual neuron responses to identify patterns and local maxima.
    * **Spatial Mapping:**  Creating spatial maps of neuronal activity to visualize the distribution of responses across the simulated cortex.


**3. Results:**

The results revealed a marked shift in the neuronal response patterns across the different image conditions:

* **Image 1 (Topographical Map):** Initial activity showed a broad, relatively uniform increase across the entire population, consistent with a baseline response to complex textural input.
* **Image 2 (Drifting Lines):** A distinct “wave” of heightened activity emerged, following the orientation of the vertically-oriented lines.  Neurons responding to the central area of the stripes exhibited the highest activation, suggesting selective orientation response.  A subtle weakening of the response occurred towards the edges of the image, indicative of orientation invariance.
* **Image 3 (Intersecting Shapes):**  The response became significantly more fragmented, characterized by a dense network of interconnected activations.  Activity clustered around intersections and sharp angles, with a general reduction in overall activity compared to previous stimuli.  The spatial organization of the response seemed to reflect the complex geometric arrangement of the shapes.

**4. Conclusions:**

These results provide compelling evidence that V1 neurons likely represent visual information by integrating *relational* spatial properties – including intersecting shapes, angles, and distortions – rather than relying solely on the encoding of low-level features. The fragmentation of the response observed with the complex shapes suggests a shift towards a more abstract, geometrically-oriented representation. The observed orientation invariance and sensitivity to spatial density further support this hypothesis, indicating a sophisticated mechanism for resolving ambiguities and extracting meaningful information from complex visual stimuli.  This aligns with emerging perspectives on V1’s role as a ‘scene graph’ processor – constructing a hierarchical representation of spatial relationships within the visual environment [azabou et al., 2023].  The observed decrease in overall activity with increasing spatial complexity reinforces the idea that V1 processing becomes computationally demanding when confronted with intricate visual forms.

**5. Future Directions:**

* **Exploring Different Spatial Architectures:** Investigating the impact of varying the spatial arrangement of elements in the generated images (e.g., randomized shapes, different intersection patterns) to further refine our understanding of V1’s encoding mechanisms.
* **Incorporating Contextual Information:**  Simulating the effects of context by presenting the generated images alongside other visual cues, to investigate how V1 integrates relational information with broader contextual information.
* **Investigating Neuron Subtypes:**  Exploring the heterogeneity of V1 neuron populations by examining the response patterns of individual neuron types to different stimuli, seeking to identify specific neuronal populations that are specialized for processing relational spatial properties.
* **Validation with In-Vivo Data:**  Extending these in silico findings by validating the predictions using recordings from mouse visual cortex, potentially combining the model's predictions with data from selective stimulation experiments.

These investigations could pave the way for a more nuanced understanding of how the visual system encodes and processes the complex, relational information that is critical for our ability to perceive and interact with the world.
